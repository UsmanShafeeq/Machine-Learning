{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9057601,"sourceType":"datasetVersion","datasetId":5461696}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparative Analysis of Machine Learning Algorithms for Rice Leaf Disease Detection: Achieving 97% Classification Accuracy","metadata":{}},{"cell_type":"markdown","source":"# Research Problem\n\nRice is one of the most important staple crops globally, and its productivity is often compromised due to various leaf diseases. Timely and accurate identification of these diseases is critical for ensuring effective crop management and preventing yield losses. Traditional methods for detecting rice leaf diseases rely on manual visual inspection, which is time-consuming, prone to errors, and often inaccessible in large-scale farming operations.\n\nMachine learning (ML) algorithms offer a promising solution by automating the classification of rice leaf diseases with high accuracy. However, the challenge lies in selecting the most effective algorithm that balances accuracy, computational efficiency, and scalability. Despite advances in ML, there is still limited comprehensive analysis comparing different ML models for rice leaf disease detection. Existing studies often focus on a single algorithm or lack the exploration of diverse techniques that could lead to more accurate and efficient solutions.\n\nThe problem this research addresses is the need for a comparative analysis of multiple machine learning algorithms to determine the most suitable model for rice leaf disease classification. The goal is to achieve a high classification accuracy (97% or more) while also considering factors such as model complexity, training time, and resource requirements.\n","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Define Paths and Parameters","metadata":{}},{"cell_type":"code","source":"DATASET_DIR = '/kaggle/input/rice-disease-dataset/Rice_Leaf_AUG'  \nIMG_SIZE = (224, 224) \nBATCH_SIZE = 32 \nSEED = 42  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Create Data Augmentation and Normalization","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1./255,  # Normalize pixel values to [0, 1]\n    rotation_range=40,  # Randomly rotate images in the range (degrees)\n    width_shift_range=0.2,  # Randomly translate images horizontally\n    height_shift_range=0.2,  # Randomly translate images vertically\n    shear_range=0.2,  # Shear angle in counter-clockwise direction (degrees)\n    zoom_range=0.2,  # Randomly zoom image\n    horizontal_flip=True,  # Randomly flip images horizontally\n    fill_mode='nearest'  # Strategy for filling in newly created pixels\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Load the Dataset Using Data Generators","metadata":{}},{"cell_type":"code","source":"# Load all data from the directory\nall_data_gen = datagen.flow_from_directory(\n    DATASET_DIR,\n    target_size=IMG_SIZE,  # Resize images to the target size\n    batch_size=BATCH_SIZE,  # Number of images to return in each batch\n    class_mode='categorical',  # Label mode (one-hot encoded labels)\n    shuffle=True,  # Shuffle data (for training)\n    seed=SEED  # Seed for reproducibility\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Visualize Augmented Images (Optional)","metadata":{}},{"cell_type":"code","source":"# Retrieve a batch of images\nbatch = next(all_data_gen)\n\n# Plot some of the images\nplt.figure(figsize=(10, 10))\nfor i in range(9):  # Display a 3x3 grid of images\n    plt.subplot(3, 3, i+1)\n    plt.imshow(batch[0][i])  # Display image\n    plt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Prepare Data for Model Training","metadata":{}},{"cell_type":"code","source":"# Split dataset into training, validation, and test sets\nfrom sklearn.model_selection import train_test_split\n\n# Create a DataFrame with image file paths and class labels\nfilenames = all_data_gen.filepaths\nclasses = all_data_gen.classes\nclass_labels = list(all_data_gen.class_indices.keys())\n\ndata = pd.DataFrame({\n    'filename': filenames,\n    'class': [class_labels[k] for k in classes]\n})\n\n# Split data into train, validation, and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, stratify=data['class'], random_state=SEED)\nval_data, test_data = train_test_split(test_data, test_size=0.5, stratify=test_data['class'], random_state=SEED)\n\n# Create data generators for training, validation, and testing\ntrain_generator = datagen.flow_from_dataframe(\n    dataframe=train_data,\n    x_col='filename',\n    y_col='class',\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    seed=SEED\n)\n\nval_generator = datagen.flow_from_dataframe(\n    dataframe=val_data,\n    x_col='filename',\n    y_col='class',\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    seed=SEED\n)\n\ntest_generator = datagen.flow_from_dataframe(\n    dataframe=test_data,\n    x_col='filename',\n    y_col='class',\n    target_size=IMG_SIZE,\n    batch_size=1,\n    class_mode='categorical',\n    shuffle=False\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode class labels for traditional ML models\nencoder = LabelEncoder()\ntrain_data['class'] = encoder.fit_transform(train_data['class'])\nval_data['class'] = encoder.transform(val_data['class'])\ntest_data['class'] = encoder.transform(test_data['class'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract features from the images for traditional ML models\ndef extract_features(generator):\n    features = []\n    labels = []\n    for i in range(len(generator)):\n        images, label = generator[i]\n        features.append(images)\n        labels.append(label)\n    return np.vstack(features), np.vstack(labels)\n\nX_train, y_train = extract_features(train_generator)\nX_val, y_val = extract_features(val_generator)\nX_test, y_test = extract_features(test_generator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the images for traditional ML algorithms\nX_train_flat = X_train.reshape(X_train.shape[0], -1)\nX_val_flat = X_val.reshape(X_val.shape[0], -1)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection - Traditional ML Models","metadata":{}},{"cell_type":"code","source":"# 1. Support Vector Machine (SVM)\nsvm = SVC(kernel='linear', random_state=SEED)\nsvm.fit(X_train_flat, np.argmax(y_train, axis=1))\ny_pred_svm = svm.predict(X_test_flat)\nprint(\"SVM Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred_svm))\nprint(\"SVM Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), y_pred_svm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=SEED)\nrf.fit(X_train_flat, np.argmax(y_train, axis=1))\ny_pred_rf = rf.predict(X_test_flat)\nprint(\"Random Forest Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred_rf))\nprint(\"Random Forest Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), y_pred_rf))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. k-Nearest Neighbors (k-NN)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_flat, np.argmax(y_train, axis=1))\ny_pred_knn = knn.predict(X_test_flat)\nprint(\"k-NN Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred_knn))\nprint(\"k-NN Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), y_pred_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Gradient Boosting Classifier\ngb = GradientBoostingClassifier(random_state=SEED)\ngb.fit(X_train_flat, np.argmax(y_train, axis=1))\ny_pred_gb = gb.predict(X_test_flat)\nprint(\"Gradient Boosting Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred_gb))\nprint(\"Gradient Boosting Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), y_pred_gb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Convolutional Neural Network (CNN) - Transfer Learning with EfficientNetB0\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=IMG_SIZE + (3,))\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(len(class_labels), activation='softmax')(x)\n\ncnn_model = Model(inputs=base_model.input, outputs=predictions)\n\n# Freeze the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n\ncnn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the CNN model\nhistory = cnn_model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    epochs=10\n)\n\n# Evaluate CNN model on test data\ncnn_loss, cnn_accuracy = cnn_model.evaluate(test_generator, steps=len(test_generator))\nprint(\"CNN Accuracy: \", cnn_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}